{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafe1cc9",
   "metadata": {},
   "source": [
    "# full_like issue reproducer\n",
    "This notebook reproduces an issue with ```full_like``` in dask_awkward. I observe that when it is used, the dask_awkward job hangs. The Dask dashboard indicates that the work has been completed, but the processes don't seem to clear and the job never finishes.\n",
    "\n",
    "After some trial and error I narrowed this down to the ```full_like``` method. If an array is built with this method the job hangs; if it is not used then it completes within a minute or so, as usual. One can set this with the flag below.\n",
    "\n",
    "This only happens with dask_awkward; without dask the job completes as usual.\n",
    "\n",
    "The notebook is a minimum reproducer from a much longer workbook that is used for teaching at the University of Oslo. Some of the inline comment and variabe names may reflect this, as may some unused data structures/variables. The relevant input file (a plain ROOT n-tuple) can be found at\n",
    "\n",
    "```\n",
    "/afs/cern.ch/work/j/jcatmore/public/dask_awkward\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use or don't use full_like\n",
    "useFullLike = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Set whether to use Dask or not\n",
    "doDask = True\n",
    "n_workers = 64 # Only relevant if Dask is used\n",
    "\n",
    "# Imports\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import uproot\n",
    "import uproot\n",
    "\n",
    "# Import Awkward Array or Dask-awkward, depending on the setting above\n",
    "if doDask:\n",
    "    import dask_awkward as ak\n",
    "else:\n",
    "    import awkward as ak\n",
    "\n",
    "# Import Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c2cb7",
   "metadata": {},
   "source": [
    "## Define some functions\n",
    "We need to define some helpful python functions for running with Dask and creating histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f25db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask cluster \n",
    "# We set the number of workers to 64 and have a single execution thread per worker\n",
    "# Thus each core independently works on its own batch of data\n",
    "# The memory per process has to be carefully chosen - too small and the job will be slowed down due to over \n",
    "# frequent reads from disk. Too large and the machine will run out of memory and start to swap data back and \n",
    "# forth from disk \n",
    "if doDask:\n",
    "    import dask\n",
    "    from dask.distributed import Client,LocalCluster\n",
    "    cluster = LocalCluster(n_workers=n_workers,processes=True,threads_per_worker=1, memory_limit=\"100GiB\")\n",
    "    client = Client(cluster)\n",
    "    client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0fd61",
   "metadata": {},
   "source": [
    "This function generates a convenient record from an Awkward Array handle and a dictionary of names and keys (it will be clear how it works later one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a record from the list of variable names\n",
    "def buildRecord(awkwardEvents,variableDict):\n",
    "    theDict = {}\n",
    "    for key in variableDict.keys():\n",
    "        if doDask: \n",
    "            theDict[key] = awkwardEvents[variableDict[key]]\n",
    "        else:\n",
    "            theDict[key] = awkwardEvents[variableDict[key]].array()\n",
    "    return ak.zip(theDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef84160",
   "metadata": {},
   "source": [
    "## Getting and preparing the input \n",
    "\n",
    "In this part we list the variables needed and point the software at the relevant input files.\n",
    "\n",
    "First we list all of the variables that we'll use in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e331ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_variables = {'scalef': 'scalef',\n",
    "                   'n_electron': 'n_electron',\n",
    "                   'n_muon': 'n_muon',\n",
    "                   'n_tau': 'n_tau',\n",
    "                   'n_photon': 'n_photon',\n",
    "                   'n_heavyjet': 'n_heavyjet',\n",
    "                   'n_lightjet': 'n_lightjet',\n",
    "                   'category': 'category'\n",
    "                  }\n",
    "\n",
    "all_variables = []\n",
    "for dictionary in [event_variables]:\n",
    "    all_variables += list(dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which processes to run... each process sits in its own file. All that is necessary is to list the\n",
    "# process names in an array, which will then be looped over during the analysis.\n",
    "# If not using Dask, just select one of them.\n",
    "\n",
    "# Open the files and create the data structures. If you are running Dask, nothing will happen at this \n",
    "# stage (lazy execution)\n",
    "path = \"/storage/shared/data/PHYSLITEforML/forCompSciFeb04/\"\n",
    "process =  \"singletop_nom_2L\"\n",
    "inputString = path+process+\".root:CollectionTree\"\n",
    "if doDask:        \n",
    "    events = uproot.dask(inputString,\n",
    "                         library=\"ak\",\n",
    "                         filter_name=all_variables,\n",
    "                         open_files=False)\n",
    "else:\n",
    "    events = uproot.open(inputString,filter_name=all_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up easy-to-use data structures\n",
    "eventLevel = buildRecord(events,event_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to be written to HDF5\n",
    "arraysForHDF5 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the arrays independent of e,m,t kinematics. \n",
    "arraysForHDF5[\"n_electron\"] = eventLevel.n_electron\n",
    "arraysForHDF5[\"n_muon\"] = eventLevel.n_muon\n",
    "arraysForHDF5[\"n_tau\"] = eventLevel.n_tau\n",
    "arraysForHDF5[\"n_photon\"] = eventLevel.n_photon\n",
    "arraysForHDF5[\"n_lightjet\"] = eventLevel.n_lightjet\n",
    "arraysForHDF5[\"n_heavyjet\"] = eventLevel.n_heavyjet\n",
    "arraysForHDF5[\"scalef\"] = eventLevel.scalef\n",
    "if useFullLike:\n",
    "    if (\"Higgs_\" in process):\n",
    "        arraysForHDF5[\"isSignal\"] = ak.full_like(arraysForHDF5[\"scalef\"],1)\n",
    "    else:\n",
    "        arraysForHDF5[\"isSignal\"] = ak.full_like(arraysForHDF5[\"scalef\"],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Dask used, compute the arrays (if run without dask they'll have already been computed)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if doDask:\n",
    "\n",
    "    arraysForEvaluation = []\n",
    "\n",
    "    for result in arraysForHDF5.keys():\n",
    "        arraysForEvaluation.append(arraysForHDF5[result])\n",
    "            \n",
    "    print(\"Number of arrays to evaluate = \",len(arraysForEvaluation))\n",
    "\n",
    "    # Evaluate the arrays\n",
    "    evaluated = dask.compute(arraysForEvaluation)\n",
    "            \n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken to execute = \",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdd3c1",
   "metadata": {},
   "source": [
    "# Monitoring of DASK jobs\n",
    "If you'd like to monitor your DASK jobs start a new terminal and open a tunnel to port 8787, which is the default port of the DASK dashbord. You can also configure the address using the *dashboard_address* parameter (see [LocalCluster](https://docs.dask.org/en/latest/deploying-python.html#distributed.deploy.local.LocalCluster)). The ssh-tunnel is set by doing in the terminal\n",
    "```\n",
    "ssh -L 8888:localhost:8787 username@hepp03.hpc.uio.no\n",
    "```\n",
    "\n",
    "Then open [http://localhost:8888/status](http://localhost:8888/status) (where you must switch 8888 with the port you chose above).\n",
    "\n",
    "More information about Dask Dashboard can be found [here](https://docs.dask.org/en/latest/dashboard.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787998a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
